# -*- coding: utf-8 -*-
"""sunhacks2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7J_E-4LLnnR8UO37Jp7Gni4rrjBn1C_
"""

import pymongo
from PIL import Image
import io
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# from PIL import Image
# import os

# # Load a sample image to get dimensions
# sample_image_path = '/content/frame_0004.jpg'  # Update the path accordingly
# sample_image = Image.open(sample_image_path)
# width, height = sample_image.size
# print(width, height)

# """width: 1280
# height: 720
# """


class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)

        # Adjust dimensions based on the input image size of 720x1280
        self.fc1 = nn.Linear(128 * 90 * 160, 128)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 128 * 90 * 160)  # Flatten
        x = F.relu(self.fc1(x))
        return x  # Return features

# MongoDB connection details
mongo_client = pymongo.MongoClient("mongodb+srv://sbuddha4:Q1w2e3r4t5y69*@movieframes.quhx9.mongodb.net/")
db = mongo_client["sunhacks2024"]  # Replace with your database name
collection = db["Movie_Frames"]  # Replace with your collection name

# Define a custom dataset that loads images from MongoDB with the folder structure
class MongoDBMovieDataset(Dataset):
    def __init__(self, collection, transform=None):
        self.collection = collection
        self.transform = transform
        self.image_data = list(self.collection.find())

    def __len__(self):
        return len(self.image_data)

    def __getitem__(self, idx):
        # Get the image document
        image_doc = self.image_data[idx]
        image_bytes = image_doc['image_data']  # Adjust the key based on your document structure
        folder_name = image_doc['movie_name']  # Assuming you have a field indicating the movie folder

        # Convert bytes to an image
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, folder_name  # Return both the image and the folder name


# Define transformations
transform = transforms.Compose([
    transforms.Resize((720, 1280)),  # Resize to desired dimensions
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

# Create the dataset and dataloader
dataset = MongoDBMovieDataset(collection, transform=transform)
dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

# Function to extract features using the CNN
def extract_features(model, dataloader):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()  # Set the model to evaluation mode

    all_features = []
    movie_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            features = model(images)  # Extract features
            all_features.append(features.cpu())  # Collect features on CPU
            movie_labels.extend(labels)  # Collect movie folder labels

    return torch.cat(all_features, dim=0), movie_labels  # Return all features and labels

model = SimpleCNN()
features, movie_labels = extract_features(model, dataloader)
print("Feature extraction complete.")
print(f"Extracted features shape: {features.shape}")

# Function to perform feature-based searching
def find_similar_images(query_image, model, features, movie_labels, top_k=1):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    # Apply transformations to the query image
    query_image = transform(query_image).unsqueeze(0).to(device)

    with torch.no_grad():
        query_features = model(query_image).cpu()  # Extract features of the query image

    # Calculate cosine similarity between the query image's features and all other features
    similarities = cosine_similarity(query_features, features)

    # Find the indices of the top-k most similar images
    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]

    # Retrieve the corresponding movie labels for the top-k similar images
    similar_movie_labels = [movie_labels[idx] for idx in top_k_indices]

    return similar_movie_labels

"""Testing"""

# query_image_path = '/content/frame_0007.jpg'  # Replace with the path to your query image
# query_image = Image.open(query_image_path).convert("RGB")

# # Find similar images
# similar_movies = find_similar_images(query_image, model, features, movie_labels, top_k=1)

# # Output the names of the top-k most similar movies
# print("Movie: ", similar_movies)


import streamlit as st
from PIL import Image
import torch

# Set the page configuration
st.set_page_config(page_title="Movie Frame Identifier", page_icon="ðŸŽ¬", layout="centered")

# Custom CSS for styling
st.markdown("""
    <style>
        .stApp {
            background-color: #FFEB3B;
        }
        .upload-text {
            font-family: 'Comic Sans MS', cursive, sans-serif;
            font-size: 24px;
            color: #333;
            text-align: center;
        }
        .button-style {
            display: flex;
            justify-content: center;
        }
    </style>
    """, unsafe_allow_html=True)

# Page title
st.markdown("<h1 style='text-align: center; color: #333; font-family:Arial Black;'>Upload Frame</h1>", unsafe_allow_html=True)

# Upload file button
uploaded_file = st.file_uploader("", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Display the uploaded image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Frame', use_column_width=True)

    # 'Find the Movie' button
    if st.button('Find the Movie'):
        st.write("Analyzing the frame, please wait...")

        # Your model inference code here
        query_image_path = image
        query_image = Image.open(query_image_path).convert("RGB")

        similar_movies = find_similar_images(query_image, model, features, movie_labels, top_k=1)

        # Display the result
        st.success(f"The frame is from the movie: **{similar_movies}**")
else:
    st.info("Please upload an image file.")


